#!/usr/bin/env python3
"""
DamaDam Profile Scraper - ENHANCED VERSION with Recent Post Data
GitHub Actions ready with Tags integration and smart updates
"""

import os
import sys
import time
import json
import random
import re
from datetime import datetime, timedelta

print("üöÄ Starting DamaDam Scraper (Enhanced with Post Data)...")

# Check required packages
missing_packages = []

try:
    from selenium import webdriver
    from selenium.webdriver.common.by import By
    from selenium.webdriver.chrome.service import Service
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.common.exceptions import TimeoutException, NoSuchElementException
    from webdriver_manager.chrome import ChromeDriverManager
    print("‚úÖ Selenium ready")
except ImportError:
    missing_packages.append("selenium webdriver-manager")

try:
    import colorama
    from colorama import Fore, Style
    colorama.init(autoreset=True)
    print("‚úÖ Colors ready")
except ImportError:
    missing_packages.append("colorama")
    class Fore:
        CYAN = GREEN = YELLOW = RED = WHITE = MAGENTA = BLUE = ""
    class Style:
        RESET_ALL = ""

try:
    import gspread
    from oauth2client.service_account import ServiceAccountCredentials
    print("‚úÖ Google Sheets ready")
except ImportError:
    missing_packages.append("gspread oauth2client")

if missing_packages:
    print(f"‚ùå Missing packages: {missing_packages}")
    sys.exit(1)

# === CONFIGURATION ===
LOGIN_URL = "https://damadam.pk/login/"
ONLINE_USERS_URL = "https://damadam.pk/online_kon/"

# Environment variables
USERNAME = os.getenv('DAMADAM_USERNAME')
PASSWORD = os.getenv('DAMADAM_PASSWORD')
SHEET_URL = os.getenv('GOOGLE_SHEET_URL')

if not all([USERNAME, PASSWORD, SHEET_URL]):
    print("‚ùå Missing required environment variables!")
    print("Required: DAMADAM_USERNAME, DAMADAM_PASSWORD, GOOGLE_SHEET_URL")
    sys.exit(1)

# Rate limiting configuration
GOOGLE_API_RATE_LIMIT = {
    'max_requests_per_minute': 50,
    'batch_size': 3,
    'retry_delay': 65,
    'request_delay': 1.2
}

# Request tracking for rate limiting
api_requests = []

def track_api_request():
    """Track API requests for rate limiting"""
    now = datetime.now()
    global api_requests
    
    api_requests = [req_time for req_time in api_requests if (now - req_time).seconds < 60]
    api_requests.append(now)
    
    if len(api_requests) >= GOOGLE_API_RATE_LIMIT['max_requests_per_minute']:
        log_msg("‚è∏Ô∏è Rate limit approaching, pausing for 65 seconds...", "WARNING")
        time.sleep(GOOGLE_API_RATE_LIMIT['retry_delay'])
        api_requests = []

# Optimized delays
MIN_DELAY = 1.0
MAX_DELAY = 2.0
LOGIN_DELAY = 4
PAGE_LOAD_TIMEOUT = 8

# Tags configuration
TAGS_CONFIG = {
    'Following': 'üîó Following',
    'Followers': '‚≠ê Followers', 
    'Bookmark': 'üìñ Bookmark',
    'Pending': '‚è≥ Pending'
}

# === LOGGING ===
def log_msg(message, level="INFO"):
    timestamp = datetime.now().strftime("%H:%M:%S")
    colors = {"INFO": Fore.WHITE, "SUCCESS": Fore.GREEN, "WARNING": Fore.YELLOW, "ERROR": Fore.RED}
    color = colors.get(level, Fore.WHITE)
    print(f"{color}[{timestamp}] {level}: {message}{Style.RESET_ALL}")

# === STATS TRACKING ===
class ScraperStats:
    def __init__(self):
        self.start_time = datetime.now()
        self.total = self.current = self.success = self.errors = 0
        self.new_profiles = self.updated_profiles = 0
        self.tags_processed = 0
        self.posts_scraped = 0
    
    def show_summary(self):
        elapsed = str(datetime.now() - self.start_time).split('.')[0]
        print(f"\n{Fore.MAGENTA}üìä FINAL SUMMARY:")
        print(f"‚è±Ô∏è  Total Time: {elapsed}")
        print(f"üéØ Target Users: {self.total}")
        print(f"‚úÖ Successfully Scraped: {self.success}")
        print(f"‚ùå Errors: {self.errors}")
        print(f"üÜï New Profiles: {self.new_profiles}")
        print(f"üîÑ Updated Profiles: {self.updated_profiles}")
        print(f"üè∑Ô∏è  Tags Processed: {self.tags_processed}")
        print(f"üìù Posts Scraped: {self.posts_scraped}")
        
        if self.total > 0:
            completion_rate = (self.success / self.total * 100)
            remaining = self.total - self.success
            print(f"üìà Completion Rate: {completion_rate:.1f}%")
            print(f"‚è≥ Remaining: {remaining} users pending")
        
        print(f"{Style.RESET_ALL}")
        print("-" * 50)

stats = ScraperStats()

# === DATE CONVERSION UTILITIES ===
def convert_relative_date_to_absolute(relative_text):
    """Convert relative dates like '2 months ago' to actual date format dd-mmm-yy"""
    if not relative_text or relative_text.strip() == "":
        return ""
    
    relative_text = relative_text.lower().strip()
    now = datetime.now()
    
    try:
        # Extract number and unit
        match = re.search(r'(\d+)\s*(second|minute|hour|day|week|month|year)s?\s*ago', relative_text)
        
        if match:
            amount = int(match.group(1))
            unit = match.group(2)
            
            if unit == 'second':
                target_date = now - timedelta(seconds=amount)
            elif unit == 'minute':
                target_date = now - timedelta(minutes=amount)
            elif unit == 'hour':
                target_date = now - timedelta(hours=amount)
            elif unit == 'day':
                target_date = now - timedelta(days=amount)
            elif unit == 'week':
                target_date = now - timedelta(weeks=amount)
            elif unit == 'month':
                # Approximate month as 30 days
                target_date = now - timedelta(days=amount * 30)
            elif unit == 'year':
                target_date = now - timedelta(days=amount * 365)
            else:
                return relative_text
            
            return target_date.strftime("%d-%b-%y")
        
        # If no match, return original text
        return relative_text
        
    except Exception as e:
        log_msg(f"‚ö†Ô∏è Date conversion error: {e}", "WARNING")
        return relative_text

def parse_post_timestamp(timestamp_text):
    """Parse post timestamp and convert to dd-mmm-yy hh:mm A/P format"""
    if not timestamp_text or timestamp_text.strip() == "":
        return "N/A"
    
    timestamp_text = timestamp_text.strip()
    now = datetime.now()
    
    try:
        # Handle relative timestamps
        match = re.search(r'(\d+)\s*(second|minute|hour|day|week|month|year)s?\s*ago', timestamp_text.lower())
        
        if match:
            amount = int(match.group(1))
            unit = match.group(2)
            
            if unit == 'second':
                target_date = now - timedelta(seconds=amount)
            elif unit == 'minute':
                target_date = now - timedelta(minutes=amount)
            elif unit == 'hour':
                target_date = now - timedelta(hours=amount)
            elif unit == 'day':
                target_date = now - timedelta(days=amount)
            elif unit == 'week':
                target_date = now - timedelta(weeks=amount)
            elif unit == 'month':
                target_date = now - timedelta(days=amount * 30)
            elif unit == 'year':
                target_date = now - timedelta(days=amount * 365)
            else:
                return timestamp_text
            
            return target_date.strftime("%d-%b-%y %I:%M %p")
        
        return timestamp_text
        
    except Exception as e:
        log_msg(f"‚ö†Ô∏è Timestamp parsing error: {e}", "WARNING")
        return timestamp_text

# === BROWSER SETUP ===
def setup_github_browser():
    """Optimized browser setup for GitHub Actions"""
    try:
        log_msg("üöÄ Setting up browser for GitHub Actions...")
        
        options = webdriver.ChromeOptions()
        
        options.add_argument("--headless=new")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-gpu")
        options.add_argument("--window-size=1920,1080")
        options.add_argument("--disable-extensions")
        options.add_argument("--disable-plugins")
        options.add_argument("--disable-default-apps")
        options.add_argument("--no-first-run")
        options.add_argument("--disable-background-networking")
        options.add_argument("--disable-sync")
        options.add_argument("--disable-translate")
        options.add_argument("--memory-pressure-off")
        options.add_argument("--max_old_space_size=4096")
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)
        options.add_argument("--log-level=3")
        
        try:
            service = Service()
            driver = webdriver.Chrome(service=service, options=options)
        except Exception:
            from webdriver_manager.chrome import ChromeDriverManager
            service = Service(ChromeDriverManager().install())
            driver = webdriver.Chrome(service=service, options=options)
        
        driver.set_page_load_timeout(15)
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        driver.execute_script("Object.defineProperty(navigator, 'plugins', {get: () => [1, 2, 3, 4, 5]})")
        
        log_msg("‚úÖ Browser ready", "SUCCESS")
        return driver
        
    except Exception as e:
        log_msg(f"‚ùå Browser setup failed: {e}", "ERROR")
        return None

# === AUTHENTICATION ===
def login_to_damadam(driver):
    """Enhanced login with comprehensive debugging"""
    try:
        log_msg("üîê Logging in to DamaDam...")
        driver.get(LOGIN_URL)
        time.sleep(3)
        
        current_url = driver.current_url
        page_title = driver.title
        log_msg(f"üîç Current URL: {current_url}", "INFO")
        log_msg(f"üìÑ Page title: {page_title}", "INFO")
        
        login_selectors = [
            {"nick": "#nick", "pass": "#pass", "button": "form button"},
            {"nick": "input[name='nick']", "pass": "input[name='pass']", "button": "button[type='submit']"},
            {"nick": "input[placeholder*='nick']", "pass": "input[type='password']", "button": ".btn"},
            {"nick": "[name='username']", "pass": "[name='password']", "button": "input[type='submit']"},
        ]
        
        form_found = False
        for i, selectors in enumerate(login_selectors):
            try:
                log_msg(f"üîç Trying login method {i+1}...", "INFO")
                
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, selectors["nick"]))
                )
                
                nick_field = driver.find_element(By.CSS_SELECTOR, selectors["nick"])
                pass_field = driver.find_element(By.CSS_SELECTOR, selectors["pass"])
                submit_btn = driver.find_element(By.CSS_SELECTOR, selectors["button"])
                
                log_msg(f"‚úÖ Found login form with method {i+1}", "SUCCESS")
                
                nick_field.clear()
                time.sleep(0.5)
                nick_field.send_keys(USERNAME)
                
                pass_field.clear()
                time.sleep(0.5)
                pass_field.send_keys(PASSWORD)
                
                nick_value = nick_field.get_attribute('value')
                pass_length = len(pass_field.get_attribute('value'))
                log_msg(f"üîç Username filled: {nick_value[:3]}***", "INFO")
                log_msg(f"üîç Password filled: {pass_length} characters", "INFO")
                
                log_msg("üöÄ Submitting login form...", "INFO")
                submit_btn.click()
                form_found = True
                break
                
            except Exception as e:
                log_msg(f"‚ö†Ô∏è Target sheet update failed: {e}", "WARNING")
        
        if not profiles_batch:
            return True
            
        # Get main worksheet
        worksheet = workbook.sheet1
        
        # Updated headers with merged DATETIME column
        headers = ["DATETIME","NICKNAME","TAGS","CITY","GENDER","MARRIED","AGE",
                   "JOINED","FOLLOWERS","POSTS","LPOST","LDATE-TIME","PLINK","PIMAGE","INTRO"]
        
        track_api_request()
        existing_data = worksheet.get_all_values()
        
        if not existing_data or not existing_data[0]: 
            track_api_request()
            worksheet.append_row(headers)
            log_msg("‚úÖ Headers added to Google Sheet (merged datetime)", "SUCCESS")
            existing_rows = {}
        else:
            existing_rows = {}
            for i, row in enumerate(existing_data[1:], 2):
                if len(row) > 1 and row[1].strip():  # Check nickname column (now at index 1)
                    existing_rows[row[1].strip()] = {
                        'row_index': i,
                        'data': row
                    }
        
        new_count = 0
        updated_count = 0
        
        # Process profiles with rate limiting
        for profile in profiles_batch:
            try:
                nickname = profile.get("NICKNAME","").strip()
                if not nickname: 
                    continue
                
                # Add tags to profile
                profile['TAGS'] = get_tags_for_nickname(nickname, tags_mapping)
                
                # Prepare row data with merged datetime
                row = [
                    profile.get("DATETIME",""),
                    nickname,
                    profile.get("TAGS",""),
                    profile.get("CITY",""),
                    profile.get("GENDER",""),
                    profile.get("MARRIED",""),
                    profile.get("AGE",""),
                    profile.get("JOINED",""),
                    profile.get("FOLLOWERS",""),
                    profile.get("POSTS",""),
                    profile.get("LPOST",""),
                    profile.get("LDATE-TIME",""),
                    profile.get("PLINK",""),
                    profile.get("PIMAGE",""),
                    clean_text(profile.get("INTRO",""))
                ]
                
                if nickname in existing_rows:
                    # Update existing profile
                    existing_info = existing_rows[nickname]
                    row_index = existing_info['row_index']
                    existing_data_row = existing_info['data']
                    
                    # Check if update needed
                    needs_update = False
                    key_fields = [3, 4, 5, 6, 7, 8, 9, 10, 11, 14]  # Key data fields
                    
                    for field_idx in key_fields:
                        existing_value = existing_data_row[field_idx] if field_idx < len(existing_data_row) else ""
                        new_value = row[field_idx] if field_idx < len(row) else ""
                        if existing_value != new_value and new_value:
                            needs_update = True
                            break
                    
                    # Check tags change
                    existing_tags = existing_data_row[2] if len(existing_data_row) > 2 else ""
                    if existing_tags != row[2]:
                        needs_update = True
                    
                    if needs_update:
                        try:
                            track_api_request()
                            
                            # Update row
                            range_name = f'A{row_index}:O{row_index}'
                            worksheet.update(range_name, [row])
                            updated_count += 1
                            stats.updated_profiles += 1
                            log_msg(f"üîÑ Updated {nickname}", "INFO")
                            
                            time.sleep(GOOGLE_API_RATE_LIMIT['request_delay'])
                            
                        except Exception as e:
                            if "429" in str(e) or "RATE_LIMIT_EXCEEDED" in str(e):
                                log_msg(f"üö® Rate limit hit updating {nickname}, retrying after 65s...", "WARNING")
                                time.sleep(65)
                                try:
                                    worksheet.update(range_name, [row])
                                    updated_count += 1
                                    log_msg(f"‚úÖ Retry successful for {nickname}", "SUCCESS")
                                except:
                                    log_msg(f"‚ùå Failed to update {nickname} after retry", "ERROR")
                            else:
                                log_msg(f"‚ùå Failed to update {nickname}: {e}", "ERROR")
                    else:
                        log_msg(f"‚û°Ô∏è {nickname} - No changes needed", "INFO")
                        
                else:
                    # Add new profile
                    try:
                        track_api_request()
                        
                        worksheet.append_row(row)
                        new_count += 1
                        stats.new_profiles += 1
                        log_msg(f"‚úÖ Added new profile: {nickname}", "SUCCESS")
                        
                        time.sleep(GOOGLE_API_RATE_LIMIT['request_delay'])
                        
                    except Exception as e:
                        if "429" in str(e) or "RATE_LIMIT_EXCEEDED" in str(e):
                            log_msg(f"üö® Rate limit hit adding {nickname}, retrying after 65s...", "WARNING")
                            time.sleep(65)
                            try:
                                worksheet.append_row(row)
                                new_count += 1
                                log_msg(f"‚úÖ Retry successful for {nickname}", "SUCCESS")
                            except:
                                log_msg(f"‚ùå Failed to add {nickname} after retry", "ERROR")
                        else:
                            log_msg(f"‚ùå Failed to add {nickname}: {e}", "ERROR")
                            
            except Exception as e:
                log_msg(f"‚ùå Error processing profile {nickname}: {e}", "ERROR")
        
        log_msg(f"üìä Rate-limited export complete: {new_count} new, {updated_count} updated", "SUCCESS")
        return True
        
    except Exception as e:
        log_msg(f"‚ùå Google Sheets export failed: {e}", "ERROR")
        return False

# === MAIN EXECUTION ===
def main():
    """Enhanced main execution with target user processing and post data"""
    log_msg("üöÄ Starting DamaDam Profile Scraper (Enhanced with Post Data)", "INFO")
    
    # Setup browser
    driver = setup_github_browser()
    if not driver:
        log_msg("‚ùå Failed to setup browser", "ERROR")
        return
    
    try:
        # Login
        if not login_to_damadam(driver):
            log_msg("‚ùå Authentication failed", "ERROR")
            return
        
        # Get Google Sheets client and tags mapping
        client = get_google_sheets_client()
        if not client:
            log_msg("‚ùå Failed to connect to Google Sheets", "ERROR")
            return
            
        tags_mapping = get_tags_mapping(client, SHEET_URL)
        
        # Get target users
        target_users = get_target_users(client, SHEET_URL)
        if not target_users:
            log_msg("‚ùå No target users found or Target sheet not configured", "ERROR")
            return
        
        stats.total = len(target_users)
        scraped_profiles = []
        target_updates = []
        batch_size = GOOGLE_API_RATE_LIMIT['batch_size']
        
        log_msg(f"üéØ Processing {stats.total} target users with rate-limited batches of {batch_size}...", "INFO")
        
        # Scrape target profiles with post data
        for i, target_user in enumerate(target_users, 1):
            stats.current = i
            nickname = target_user['username']
            row_index = target_user['row_index']
            
            log_msg(f"üîç Scraping target user: {nickname} ({i}/{stats.total})", "INFO")
            
            try:
                profile = scrape_profile(driver, nickname)
                
                if profile:
                    scraped_profiles.append(profile)
                    stats.success += 1
                    
                    # Mark as completed in target updates
                    target_updates.append({
                        'row_index': row_index,
                        'status': 'Completed',
                        'notes': 'Successfully scraped with post data'
                    })
                    
                    log_msg(f"‚úÖ Successfully scraped: {nickname}", "SUCCESS")
                    
                else:
                    stats.errors += 1
                    target_updates.append({
                        'row_index': row_index,
                        'status': 'Pending',
                        'notes': 'Scraping failed - will retry'
                    })
                    log_msg(f"‚ùå Failed to scrape: {nickname}", "ERROR")
                
                # Export in smaller batches
                if len(scraped_profiles) >= batch_size or len(target_updates) >= batch_size:
                    log_msg(f"üì§ Exporting batch of {len(scraped_profiles)} profiles...", "INFO")
                    export_to_google_sheets_with_rate_limiting(scraped_profiles, tags_mapping, target_updates)
                    scraped_profiles = []
                    target_updates = []
                    
                    log_msg("‚è∏Ô∏è Pausing 10 seconds between batches for rate limit safety...", "INFO")
                    time.sleep(10)
                    
            except Exception as e:
                stats.errors += 1
                log_msg(f"‚ùå Error processing {nickname}: {e}", "ERROR")
                
                target_updates.append({
                    'row_index': row_index,
                    'status': 'Pending',
                    'notes': f'Error: {str(e)[:100]}'
                })
            
            # Smart delay
            delay = random.uniform(MIN_DELAY, MAX_DELAY)
            time.sleep(delay)
        
        # Export remaining profiles and updates
        if scraped_profiles or target_updates:
            log_msg(f"üì§ Exporting final batch of {len(scraped_profiles)} profiles...", "INFO")
            export_to_google_sheets_with_rate_limiting(scraped_profiles, tags_mapping, target_updates)
        
        # Final summary
        stats.show_summary()
        
        # Show target completion status
        completed = stats.success
        total_targets = stats.total
        completion_rate = (completed / total_targets * 100) if total_targets > 0 else 0
        
        log_msg(f"üéØ Target Processing Complete:", "INFO")
        log_msg(f"   Completed: {completed}/{total_targets} ({completion_rate:.1f}%)", "INFO")
        log_msg(f"   Remaining: {total_targets - completed} users still pending", "INFO")
        log_msg(f"   Posts Scraped: {stats.posts_scraped}", "INFO")
        
    except KeyboardInterrupt:
        log_msg("‚èπÔ∏è Scraping interrupted by user", "WARNING")
    except Exception as e:
        log_msg(f"‚ùå Execution error: {e}", "ERROR")
    finally:
        try:
            driver.quit()
        except:
            pass
        log_msg("üèÅ Scraper completed", "INFO")

if __name__ == "__main__":
    main()
                log_msg(f"‚ö†Ô∏è Login method {i+1} failed: {e}", "WARNING")
                continue
        
        if not form_found:
            log_msg("‚ùå No login form found with any method", "ERROR")
            return False
        
        log_msg("‚è≥ Waiting for login to process...", "INFO")
        time.sleep(LOGIN_DELAY)
        
        current_url_after = driver.current_url
        log_msg(f"üîç URL after login: {current_url_after}", "INFO")
        
        success_indicators = [
            lambda: "login" not in driver.current_url.lower(),
            lambda: "dashboard" in driver.current_url.lower() or "profile" in driver.current_url.lower(),
            lambda: any(driver.find_elements(By.CSS_SELECTOR, selector) for selector in [
                "[href*='logout']", "[href*='profile']", ".user-menu", ".logout"
            ]),
            lambda: not any(driver.find_elements(By.CSS_SELECTOR, selector) for selector in [
                "#nick", "input[name='nick']", ".login-form"
            ])
        ]
        
        login_success = False
        for i, check in enumerate(success_indicators):
            try:
                if check():
                    log_msg(f"‚úÖ Login success indicator {i+1} passed", "SUCCESS")
                    login_success = True
                    break
            except Exception as e:
                log_msg(f"‚ö†Ô∏è Success check {i+1} failed: {e}", "WARNING")
        
        if login_success:
            log_msg("‚úÖ Login successful!", "SUCCESS")
            return True
        else:
            log_msg("‚ùå Login failed", "ERROR")
            return False
            
    except Exception as e:
        log_msg(f"‚ùå Login error: {e}", "ERROR")
        return False

# === TARGET USERS MANAGEMENT ===
def get_target_users(client, sheet_url):
    """Get target users from Target sheet"""
    try:
        log_msg("üéØ Loading target users from Target sheet...")
        workbook = client.open_by_url(sheet_url)
        
        try:
            target_sheet = workbook.worksheet("Target")
        except:
            log_msg("‚ùå Target sheet not found! Please create 'Target' sheet", "ERROR")
            return []
        
        target_data = target_sheet.get_all_values()
        if not target_data or len(target_data) < 2:
            log_msg("‚ö†Ô∏è Target sheet is empty or has no data rows", "WARNING")
            return []
        
        headers = target_data[0]
        if len(headers) < 2 or 'USERNAME' not in headers[0].upper() or 'STATUS' not in headers[1].upper():
            log_msg("‚ùå Target sheet headers incorrect. Expected: USERNAME | STATUS | LAST_SCRAPED | NOTES", "ERROR")
            return []
        
        pending_users = []
        for i, row in enumerate(target_data[1:], 2):
            if len(row) >= 2:
                username = row[0].strip()
                status = row[1].strip().upper()
                
                if username and status == 'PENDING':
                    pending_users.append({
                        'username': username,
                        'row_index': i,
                        'status': status
                    })
        
        log_msg(f"‚úÖ Found {len(pending_users)} pending users to scrape", "SUCCESS")
        return pending_users
        
    except Exception as e:
        log_msg(f"‚ùå Failed to load target users: {e}", "ERROR")
        return []

# === RECENT POST SCRAPING ===
def scrape_recent_post(driver, nickname):
    """Scrape the most recent post from user's public profile"""
    post_url = f"https://damadam.pk/profile/public/{nickname}"
    try:
        log_msg(f"üìù Scraping recent post for {nickname}...", "INFO")
        driver.get(post_url)
        
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "article.mbl.bas-sh"))
        )
        
        recent_post = driver.find_element(By.CSS_SELECTOR, "article.mbl.bas-sh")
        
        post_data = {
            'LPOST': '',
            'LDATE-TIME': ''
        }
        
        # Extract post text
        post_text_selectors = [
            "span[style='font-size:1.1em'] bdi",
            "h2[itemprop='text']",
            "span[style*='font-size:1.1em'] bdi",
            ".post-content",
            "bdi"
        ]
        
        for selector in post_text_selectors:
            try:
                text_elem = recent_post.find_element(By.CSS_SELECTOR, selector)
                if text_elem.text.strip():
                    post_text = clean_text(text_elem.text)
                    post_data['LPOST'] = post_text[:100] + "..." if len(post_text) > 100 else post_text
                    break
            except:
                continue
        
        if not post_data['LPOST']:
            try:
                img_elem = recent_post.find_element(By.CSS_SELECTOR, "img[itemprop='image']")
                if img_elem:
                    post_data['LPOST'] = "[Image Post]"
            except:
                post_data['LPOST'] = "[No Content]"
        
        # Extract timestamp and convert to dd-mmm-yy hh:mm A/P
        timestamp_selectors = [
            "time[itemprop='datePublished']",
            "time",
            ".timestamp",
            ".post-time"
        ]
        
        for selector in timestamp_selectors:
            try:
                time_elem = recent_post.find_element(By.CSS_SELECTOR, selector)
                if time_elem.text.strip():
                    raw_timestamp = clean_text(time_elem.text)
                    post_data['LDATE-TIME'] = parse_post_timestamp(raw_timestamp)
                    break
            except:
                continue
        
        if not post_data['LDATE-TIME']:
            post_data['LDATE-TIME'] = "N/A"
        
        stats.posts_scraped += 1
        log_msg(f"‚úÖ Recent post scraped: {post_data['LPOST'][:30]}... ({post_data['LDATE-TIME']})", "SUCCESS")
        return post_data
        
    except TimeoutException:
        log_msg(f"‚è≥ No posts found for {nickname}", "WARNING")
        return {'LPOST': '[No Posts]', 'LDATE-TIME': 'N/A'}
    except Exception as e:
        log_msg(f"‚ùå Failed to scrape recent post for {nickname}: {e}", "ERROR")
        return {'LPOST': '[Error]', 'LDATE-TIME': 'N/A'}

# === PROFILE SCRAPING ===
def scrape_profile(driver, nickname):
    """Enhanced profile scraping with recent post data"""
    url = f"https://damadam.pk/users/{nickname}/"
    try:
        driver.get(url)
        
        WebDriverWait(driver, 12).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "h1.cxl.clb.lsp"))
        )
        
        now = datetime.now()
        data = {
            'DATETIME': now.strftime("%d-%b-%y %I:%M %p"),  # Merged datetime
            'NICKNAME': nickname,
            'TAGS': '',
            'CITY': '',
            'GENDER': '',
            'MARRIED': '',
            'AGE': '',
            'JOINED': '',
            'FOLLOWERS': '',
            'POSTS': '',
            'LPOST': '',
            'LDATE-TIME': '',
            'PLINK': url,
            'PIMAGE': '',
            'INTRO': ''
        }
        
        # Extract intro
        intro_selectors = [".ow span.nos", ".ow .nos", "span.nos"]
        for selector in intro_selectors:
            try:
                intro_elem = driver.find_element(By.CSS_SELECTOR, selector)
                if intro_elem.text.strip():
                    data['INTRO'] = clean_text(intro_elem.text)
                    break
            except:
                continue
            
        # Extract profile fields
        fields_mapping = {
            'City:': 'CITY',
            'Gender:': 'GENDER', 
            'Married:': 'MARRIED',
            'Age:': 'AGE',
            'Joined:': 'JOINED'
        }
        
        for field_text, key in fields_mapping.items():
            try:
                xpath_patterns = [
                    f"//b[contains(text(), '{field_text}')]/following-sibling::span[1]",
                    f"//strong[contains(text(), '{field_text}')]/following-sibling::span[1]",
                    f"//*[contains(text(), '{field_text}')]/following-sibling::span[1]"
                ]
                
                for xpath in xpath_patterns:
                    try:
                        element = driver.find_element(By.XPATH, xpath)
                        value = element.text.strip()
                        if value:
                            if key == "JOINED":
                                # Convert relative date to absolute
                                data[key] = convert_relative_date_to_absolute(value)
                            else:
                                data[key] = clean_text(value)
                            break
                    except:
                        continue
            except:
                pass
                
        # Extract followers
        follower_selectors = ["span.cl.sp.clb", ".cl.sp.clb", "span[class*='cl'][class*='sp']"]
        for selector in follower_selectors:
            try:
                followers_elem = driver.find_element(By.CSS_SELECTOR, selector)
                followers_match = re.search(r'(\d+)', followers_elem.text)
                if followers_match:
                    data['FOLLOWERS'] = followers_match.group(1)
                    break
            except:
                continue
            
        # Extract posts count from profile page
        posts_selectors = [
            "a[href*='/profile/public/'] button div:first-child",
            "a[href*='/profile/public/'] button div",
            "a[href*='profile'] button div:first-child"
        ]
        for selector in posts_selectors:
            try:
                posts_elem = driver.find_element(By.CSS_SELECTOR, selector)
                posts_text = clean_text(posts_elem.text)
                # Extract only numbers
                posts_match = re.search(r'(\d+)', posts_text)
                if posts_match:
                    data['POSTS'] = posts_match.group(1)
                    break
            except:
                continue
            
        # Extract profile image
        img_selectors = ["img[src*='avatar-imgs']", "img[src*='avatar']", ".profile-img img"]
        for selector in img_selectors:
            try:
                img_elem = driver.find_element(By.CSS_SELECTOR, selector)
                data['PIMAGE'] = img_elem.get_attribute('src')
                break
            except:
                continue
        
        # Scrape recent post data
        log_msg(f"üìù Getting recent post data for {nickname}...", "INFO")
        time.sleep(1)
        
        post_data = scrape_recent_post(driver, nickname)
        data['LPOST'] = post_data['LPOST']
        data['LDATE-TIME'] = post_data['LDATE-TIME']
            
        return data
        
    except Exception as e:
        log_msg(f"‚ùå Failed to scrape {nickname}: {e}", "ERROR")
        return None

# === UTILITY FUNCTIONS ===
def clean_text(text):
    """Enhanced text cleaning"""
    if not text: 
        return ""
    text = str(text).strip().replace('\xa0', ' ').replace('+', '').replace('\n', ' ')
    
    placeholder_texts = ['not set', 'no set', 'no city', 'none', 'n/a', 'null']
    if text.lower() in placeholder_texts: 
        return ""
    
    return re.sub(r'\s+', ' ', text).strip()

def extract_numbers(text):
    """Extract numbers from text"""
    if not text: 
        return ""
    numbers = re.findall(r'\d+', str(text))
    return ', '.join(numbers) if numbers else clean_text(text)

# === GOOGLE SHEETS OPERATIONS ===
def get_google_sheets_client():
    """Setup Google Sheets client"""
    try:
        google_creds = os.getenv('GOOGLE_SERVICE_ACCOUNT_JSON')
        if not google_creds:
            raise Exception("Missing GOOGLE_SERVICE_ACCOUNT_JSON")
            
        creds_dict = json.loads(google_creds)
        scope = ["https://spreadsheets.google.com/feeds","https://www.googleapis.com/auth/drive"]
        creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)
        client = gspread.authorize(creds)
        
        return client
    except Exception as e:
        log_msg(f"‚ùå Failed to setup Google Sheets client: {e}", "ERROR")
        return None

def get_tags_mapping(client, sheet_url):
    """Get tags mapping from Tags sheet"""
    try:
        log_msg("üè∑Ô∏è Loading tags mapping...")
        workbook = client.open_by_url(sheet_url)
        
        try:
            tags_sheet = workbook.worksheet("Tags")
        except:
            log_msg("‚ö†Ô∏è Tags sheet not found, skipping tags", "WARNING")
            return {}
        
        tags_data = tags_sheet.get_all_values()
        if not tags_data:
            return {}
        
        tags_mapping = {}
        headers = tags_data[0] if tags_data else []
        
        for col_index, header in enumerate(headers):
            if header.strip():
                tag_icon = TAGS_CONFIG.get(header.strip(), f"üîå {header.strip()}")
                
                for row in tags_data[1:]:
                    if col_index < len(row) and row[col_index].strip():
                        nickname = row[col_index].strip()
                        if nickname not in tags_mapping:
                            tags_mapping[nickname] = []
                        tags_mapping[nickname].append(tag_icon)
        
        stats.tags_processed = len(tags_mapping)
        log_msg(f"‚úÖ Loaded tags for {len(tags_mapping)} users", "SUCCESS")
        return tags_mapping
        
    except Exception as e:
        log_msg(f"‚ùå Failed to load tags: {e}", "ERROR")
        return {}

def get_tags_for_nickname(nickname, tags_mapping):
    """Get tags string for a nickname"""
    if not tags_mapping or nickname not in tags_mapping:
        return ""
    
    tags = tags_mapping[nickname]
    return ", ".join(tags) if tags else ""

def export_to_google_sheets_with_rate_limiting(profiles_batch, tags_mapping, target_updates=None):
    """Rate-limited Google Sheets export with merged datetime"""
    if not profiles_batch and not target_updates:
        return False
        
    try:
        log_msg(f"üìä Processing Google Sheets updates (Rate-Limited Mode)...", "INFO")
        
        client = get_google_sheets_client()
        if not client:
            return False
            
        workbook = client.open_by_url(SHEET_URL)
        
        # Handle target status updates
        if target_updates:
            try:
                target_sheet = workbook.worksheet("Target")
                successful_updates = 0
                
                for update in target_updates:
                    try:
                        track_api_request()
                        
                        row_idx = update['row_index']
                        status = update['status']
                        notes = update.get('notes', '')
                        
                        update_range = f'B{row_idx}:D{row_idx}'
                        update_values = [status]
                        
                        if status.upper() == 'COMPLETED':
                            update_values.append(datetime.now().strftime("%Y-%m-%d %H:%M"))
                        else:
                            update_values.append('')
                            
                        update_values.append(notes)
                        
                        target_sheet.update(update_range, [update_values])
                        successful_updates += 1
                        
                        time.sleep(GOOGLE_API_RATE_LIMIT['request_delay'])
                        
                    except Exception as e:
                        if "429" in str(e) or "RATE_LIMIT_EXCEEDED" in str(e):
                            log_msg("üö® Rate limit hit, waiting 65 seconds...", "WARNING")
                            time.sleep(65)
                            try:
                                target_sheet.update(update_range, [update_values])
                                successful_updates += 1
                            except:
                                log_msg(f"‚ùå Failed to update target after retry: {row_idx}", "ERROR")
                        else:
                            log_msg(f"‚ùå Target update error: {e}", "ERROR")
                
                log_msg(f"‚úÖ Updated {successful_updates}/{len(target_updates)} target statuses", "SUCCESS")
                
            except Exception as e: